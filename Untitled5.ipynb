{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018d42a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "     ------------------------------------ 232.6/232.6 kB 258.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\chait\\anaconda3\\lib\\site-packages (from PyPDF2) (4.12.2)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b42b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd1c6672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 35\n",
      " \n",
      " \n",
      " Development  Plan for Greater Mumbai 2014‐2034                                                                                                                                                                                                                                                      \n",
      "Acknowledgements  \n",
      "The Consultant  wishes to thank the following  individuals  from the Municipal  Corporation  of \n",
      "Greater Mumbai for their invaluable  support, insights and contributions  towards ‘Working  Paper 1 \n",
      "– Preparation  of Base Map’ for the preparation  of the Development  Plan for Greater Mumbai \n",
      "2014‐34. \n",
      " Mr. Subodh Kumar, IAS, Municipal  Commissioner;  \n",
      " Mr. Rajeev Kuknoor, Chief Engineer Development  Plan; \n",
      " Mr. Sudhir Ghate, Deputy Chief Engineer Development  Plan; \n",
      " Mr. A.G. Marathe, Deputy Chief Engineer Development  Plan; \n",
      " Mr. R. Balachandran,  Executive  Engineer and Town Planning Officer, Development  Plan. \n",
      " Our gratitude  to the following  experts for their invaluable  insights and support: \n",
      " \n",
      "Mr. V.K Phatak, Former Chief Town Planner (MMRDA);  \n",
      " Mr. A.N Kale, Former Chief Engineer, (DP); \n",
      " Mr. A. S Jain Former Dy. Chief Engineer, (DP). \n",
      " We wish to especially  thank MCGM officers, Mr. Jagdish Talreja, Mr. Dinesh Naik, Mr. Hiren \n",
      "Daftardar,  Ms. Anita Naik for their continual  support since the\n",
      " beginning  of the project and their \n",
      "help towards familiarization  and data collection.  They have been instrumental  in helping to \n",
      "contact various MCGM departments  as well as in helping to establish contact with personnel  from \n",
      "other government  departments  and organizations.  Many thanks for the MCGM team, for \n",
      "deploying  personnel,  particularly  Mr. Prasad Gharat, on extensive  field visits that have helped in \n",
      "understanding  actual ground conditions.  \n",
      " \n",
      "We apologize  if we have inadvertently  omitted anyone to whom acknowledgement  is due. We hope \n",
      "and anticipate  the work's usefulness  for the intended purpose. \n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Creating a pdf file object\n",
    "pdf = open(\"file1.pdf\",\"rb\")\n",
    "\n",
    "#creating pdf reader object\n",
    "pdf_reader = PyPDF2.PdfReader(pdf)\n",
    "\n",
    "#checking number of pages in a pdf file\n",
    "print(\"Number of pages:\",len(pdf_reader.pages))\n",
    "\n",
    "#creating a page object\n",
    "page = pdf_reader.pages[1]\n",
    "\n",
    "#finally extracting text from the page\n",
    "print(page.extract_text())\n",
    "\n",
    "#closing the pdf file\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef326f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2, urllib , nltk\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb72b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wFile = urllib.request.urlopen('http://www.udri.org/pdf/02%20working%20paper%201.pdf')\n",
    "pdfreader = PyPDF2.PdfReader(BytesIO(wFile.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b7cce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting page 2 of the docuemnt\n",
    "pageObj = pdfreader.pages[2]\n",
    "page2 = pageObj.extract_text()\n",
    "\n",
    "#Cleaning the text\n",
    "punctuations = ['(',')',';',':','[',']',',','...','.']\n",
    "tokens = word_tokenize(page2)\n",
    "stop_words = stopwords.words('english')\n",
    "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b720c96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '2014‐2034',\n",
       " 'Table',\n",
       " 'Contents',\n",
       " 'The',\n",
       " 'Consultant',\n",
       " 'wishes',\n",
       " 'thank',\n",
       " 'following',\n",
       " 'individuals',\n",
       " 'Municipal',\n",
       " 'Corporation',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " 'invaluable',\n",
       " 'support',\n",
       " 'insights',\n",
       " 'contributions',\n",
       " 'towards',\n",
       " '‘',\n",
       " 'Working',\n",
       " 'Paper',\n",
       " '1',\n",
       " '–',\n",
       " 'Preparation',\n",
       " 'Base',\n",
       " 'Map',\n",
       " '’',\n",
       " 'preparation',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '2014‐34',\n",
       " '.............................................................................................................................',\n",
       " '..............',\n",
       " '3',\n",
       " 'Our',\n",
       " 'gratitude',\n",
       " 'following',\n",
       " 'experts',\n",
       " 'invaluable',\n",
       " 'insights',\n",
       " 'support',\n",
       " '............................',\n",
       " '3',\n",
       " 'We',\n",
       " 'wish',\n",
       " 'especially',\n",
       " 'thank',\n",
       " 'MCGM',\n",
       " 'officers',\n",
       " 'Mr.',\n",
       " 'Jagdish',\n",
       " 'Talreja',\n",
       " 'Mr.',\n",
       " 'Dinesh',\n",
       " 'Naik',\n",
       " 'Mr.',\n",
       " 'Hiren',\n",
       " 'Daftardar',\n",
       " 'Ms.',\n",
       " 'Anita',\n",
       " 'Naik',\n",
       " 'continual',\n",
       " 'support',\n",
       " 'since',\n",
       " 'beginning',\n",
       " 'project',\n",
       " 'help',\n",
       " 'towards',\n",
       " 'familiarization',\n",
       " 'data',\n",
       " 'collection',\n",
       " 'They',\n",
       " 'instrumental',\n",
       " 'helping',\n",
       " 'contact',\n",
       " 'various',\n",
       " 'MCGM',\n",
       " 'departments',\n",
       " 'well',\n",
       " 'helping',\n",
       " 'establish',\n",
       " 'contact',\n",
       " 'personnel',\n",
       " 'government',\n",
       " 'departments',\n",
       " 'organizations',\n",
       " 'Many',\n",
       " 'thanks',\n",
       " 'MCGM',\n",
       " 'team',\n",
       " 'deploying',\n",
       " 'personnel',\n",
       " 'particularly',\n",
       " 'Mr.',\n",
       " 'Prasad',\n",
       " 'Gharat',\n",
       " 'extensive',\n",
       " 'field',\n",
       " 'visits',\n",
       " 'helped',\n",
       " 'understanding',\n",
       " 'actual',\n",
       " 'ground',\n",
       " 'conditions',\n",
       " '........................................................................................',\n",
       " '3',\n",
       " 'BEST',\n",
       " '...............................................................................................................................',\n",
       " '.................',\n",
       " '5',\n",
       " 'Brihanmumbai',\n",
       " 'Electric',\n",
       " 'Supply',\n",
       " 'Transport',\n",
       " 'Undertaking',\n",
       " '..............................................................',\n",
       " '5',\n",
       " 'CIDCO',\n",
       " '...............................................................................................................................',\n",
       " '..............',\n",
       " '5',\n",
       " 'City',\n",
       " 'Industrial',\n",
       " 'Development',\n",
       " 'Corporation',\n",
       " '...............................................................................',\n",
       " '5',\n",
       " 'CTP',\n",
       " '...............................................................................................................................',\n",
       " '..................',\n",
       " '5',\n",
       " 'Comprehensive',\n",
       " 'Transportation',\n",
       " 'Plan',\n",
       " '...............................................................................................',\n",
       " '5',\n",
       " 'DP',\n",
       " '...............................................................................................................................',\n",
       " '....................',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " '..........................................................................................................................',\n",
       " '5',\n",
       " 'DPGM34',\n",
       " '...............................................................................................................................',\n",
       " '..........',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '2034',\n",
       " '.......................................................................................',\n",
       " '5',\n",
       " 'DCR',\n",
       " '...............................................................................................................................',\n",
       " '..................',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Control',\n",
       " 'Regulations',\n",
       " '...................................................................................................',\n",
       " '5',\n",
       " 'DGPS',\n",
       " '...........................................................................................................................',\n",
       " '....................',\n",
       " '5',\n",
       " 'Digital',\n",
       " 'Global',\n",
       " 'Positioning',\n",
       " 'System',\n",
       " '...................................................................................................',\n",
       " '5',\n",
       " 'DPGM',\n",
       " '...............................................................................................................................',\n",
       " '..............',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '...........................................................................................',\n",
       " '5',\n",
       " 'ELU',\n",
       " '...............................................................................................................................',\n",
       " '..................',\n",
       " '5',\n",
       " 'Existing',\n",
       " 'Land',\n",
       " 'use',\n",
       " '.............................................................................................................................',\n",
       " '5',\n",
       " 'FSI',\n",
       " '...............................................................................................................................',\n",
       " '....................',\n",
       " '5',\n",
       " 'Floor',\n",
       " 'Space',\n",
       " 'Index',\n",
       " '............................................................................................................................',\n",
       " '5',\n",
       " 'GIS',\n",
       " '...............................................................................................................................',\n",
       " '...................',\n",
       " '5']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "469f4460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.Jagdish Talreja', 'Mr.Dinesh Naik', 'Mr.Hiren Daftardar', 'Ms.Anita Naik', 'Mr.Prasad Gharat']\n"
     ]
    }
   ],
   "source": [
    "name_list = list()\n",
    "check =  ['Mr.', 'Mrs.', 'Ms.']\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token.startswith(tuple(check)) and idx < (len(tokens)-1):\n",
    "        name = token + tokens[idx+1] + ' ' +  tokens[idx+2]\n",
    "        name_list.append(name)\n",
    "print(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98d6cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a word file object\n",
    "doc = open(\"Literature Survey.docx\",\"rb\")\n",
    "#creating word reader object\n",
    "document = docx.Document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b762e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2457fe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dept. of AI & ML III Year – II SemesterApplication Development – Natural Language Processing & IOT ExploreSection: Gamma (GT-03)Literature SurveySymptom Solver – Disease Diagnosis using NLPAbstractThe purpose of this literature survey is to explore existing systems, methodologies, and research related to the development of a symptom solver for disease diagnosis using Natural Language Processing (NLP).The survey investigates relevant datasets, tools, and frameworks, highlighting challenges, gaps, and opportunities for innovation. This document provides a foundation for the development of an application that automates symptom recognition and maps them to potential diseases using advanced NLP techniques.IntroductionHealthcare systems worldwide are transitioning toward automation and digital solutions for diagnosis and treatment. Symptom solvers are essential tools for reducing the burden on healthcare providers by offering preliminary diagnoses.NLP techniques can interpret natural language inputs from users to extract relevant symptoms and map them to possible diseases. This survey delves into existing research, applications, and tools to identify gaps and establish the direction for the proposed application.Related WorkExisting ApplicationsWebMD Symptom Checker: Provides disease suggestions based on user- reported symptoms but lacks personalized predictions.Ada Health: Uses AI to predict diseases with a conversational interface.Symptomate: Offers a detailed symptom analysis but is limited by rule-based systems.Research in NLP for Disease Diagnosis\"Natural Language Processing in Healthcare\": Explores NLP applications in extracting medical knowledge from unstructured text.\"Symptom Recognition and Disease Prediction Using Machine Learning\": Demonstrates the use of supervised learning for diagnosis.\"Clinical NLP: Extracting Medical Knowledge from Text\": Discusses text mining in clinical notes for symptom extraction.Technologies and MethodsNLP TechniquesNamed Entity Recognition (NER): Identifies medical entities such as symptoms, diseases, and treatments.Text Classification: Categorizes input text based on symptom presence.Relation Extraction: Links symptoms to diseases.Pre-trained Models: BioBERT, ClinicalBERT for domain-specific language understanding.Machine Learning ApproachesRule-Based Systems: Use predefined symptom-disease mappings.Supervised Learning: Train classifiers like Random Forest or SVM for disease prediction.Deep Learning: Use RNNs, LSTMs, or Transformers for sequential symptom analysis.Symptom-Disease MappingKnowledge Graphs: Tools like UMLS or SNOMED-CT to connect symptoms and diseases.Ontology-Based Mapping: Use structured vocabularies for accurate associations.DatasetsSymCat: Dataset linking symptoms to diseases.MIMIC-III: Medical records dataset for NLP tasks.PubMed: Research articles for training language models.UMLS: Comprehensive medical terminology resource.Gaps and ChallengesAmbiguity in Symptom ReportingUsers may report symptoms in varied forms (e.g., \"headache\" vs. \"pain in the head\").Synonym and polysemy challenges in symptom interpretation.Multilingual SupportLimited models for non-English symptom reporting.Lack of Comprehensive DatasetsExisting datasets may lack diversity or coverage for rare diseases.Ethical and Privacy ConcernsEnsuring compliance with HIPAA, GDPR, and other regulations for sensitive health data.Real-Time AccuracyAchieving high accuracy in real-time with limited computational resources.Proposed Solution ScopeObjectivesDevelop a scalable NLP-based system for symptom recognition.Use advanced machine learning models for disease prediction.Incorporate multilingual and conversational capabilities.Address privacy and ethical concerns through secure data handling. FeaturesUser-friendly interface for symptom input.Real-time processing and diagnosis.Feedback mechanism for iterative improvements.ConclusionThe literature survey highlights the potential of NLP in symptom analysis and disease diagnosis. By addressing gaps in datasets, multilingual support, and real-time accuracy, the proposed system aims to improve preliminary diagnosis efficiency.Future work includes implementing the application and validating it through real- world testing.\n"
     ]
    }
   ],
   "source": [
    "docu=''\n",
    "for para in document.paragraphs:\n",
    "    docu += para.text\n",
    "print(docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1097ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\chait\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\chait\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chait\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbe04fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01127d0a",
   "metadata": {},
   "source": [
    "response = urllib2.urlopen(\"https://en.wikipedia.org/wiki/narural_language_processing')\n",
    "html_doc=response.read("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fef351e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content of the paragraph 0 is ：\n",
      "\n",
      "The content of the paragraph 1 is ：\n",
      "\n",
      "The content of the paragraph 2 is ：\n",
      "\n",
      "The content of the paragraph 3 is ：\n",
      "\n",
      "The content of the paragraph 4 is ：\n",
      "\n",
      "The content of the paragraph 5 is ：\n",
      "\n",
      "The content of the paragraph 6 is ：Dept. of AI & ML III Year – II Semester\n",
      "\n",
      "The content of the paragraph 7 is ：Application Development – Natural Language Processing & IOT Explore\n",
      "\n",
      "The content of the paragraph 8 is ：Section: Gamma (GT-03)\n",
      "\n",
      "The content of the paragraph 9 is ：\n",
      "\n",
      "The content of the paragraph 10 is ：Literature Survey\n",
      "\n",
      "The content of the paragraph 11 is ：Symptom Solver – Disease Diagnosis using NLP\n",
      "\n",
      "The content of the paragraph 12 is ：Abstract\n",
      "\n",
      "The content of the paragraph 13 is ：The purpose of this literature survey is to explore existing systems, methodologies, and research related to the development of a symptom solver for disease diagnosis using Natural Language Processing (NLP).\n",
      "\n",
      "The content of the paragraph 14 is ：The survey investigates relevant datasets, tools, and frameworks, highlighting challenges, gaps, and opportunities for innovation. This document provides a foundation for the development of an application that automates symptom recognition and maps them to potential diseases using advanced NLP techniques.\n",
      "\n",
      "The content of the paragraph 15 is ：\n",
      "\n",
      "The content of the paragraph 16 is ：Introduction\n",
      "\n",
      "The content of the paragraph 17 is ：Healthcare systems worldwide are transitioning toward automation and digital solutions for diagnosis and treatment. Symptom solvers are essential tools for reducing the burden on healthcare providers by offering preliminary diagnoses.\n",
      "\n",
      "The content of the paragraph 18 is ：NLP techniques can interpret natural language inputs from users to extract relevant symptoms and map them to possible diseases. This survey delves into existing research, applications, and tools to identify gaps and establish the direction for the proposed application.\n",
      "\n",
      "The content of the paragraph 19 is ：\n",
      "\n",
      "The content of the paragraph 20 is ：\n",
      "\n",
      "The content of the paragraph 21 is ：\n",
      "\n",
      "The content of the paragraph 22 is ：\n",
      "\n",
      "The content of the paragraph 23 is ：\n",
      "\n",
      "The content of the paragraph 24 is ：\n",
      "\n",
      "The content of the paragraph 25 is ：\n",
      "\n",
      "The content of the paragraph 26 is ：Related Work\n",
      "\n",
      "The content of the paragraph 27 is ：Existing Applications\n",
      "\n",
      "The content of the paragraph 28 is ：WebMD Symptom Checker: Provides disease suggestions based on user- reported symptoms but lacks personalized predictions.\n",
      "\n",
      "The content of the paragraph 29 is ：Ada Health: Uses AI to predict diseases with a conversational interface.\n",
      "\n",
      "The content of the paragraph 30 is ：Symptomate: Offers a detailed symptom analysis but is limited by rule-based systems.\n",
      "\n",
      "The content of the paragraph 31 is ：Research in NLP for Disease Diagnosis\n",
      "\n",
      "The content of the paragraph 32 is ：\"Natural Language Processing in Healthcare\": Explores NLP applications in extracting medical knowledge from unstructured text.\n",
      "\n",
      "The content of the paragraph 33 is ：\"Symptom Recognition and Disease Prediction Using Machine Learning\": Demonstrates the use of supervised learning for diagnosis.\n",
      "\n",
      "The content of the paragraph 34 is ：\"Clinical NLP: Extracting Medical Knowledge from Text\": Discusses text mining in clinical notes for symptom extraction.\n",
      "\n",
      "The content of the paragraph 35 is ：\n",
      "\n",
      "The content of the paragraph 36 is ：\n",
      "\n",
      "The content of the paragraph 37 is ：Technologies and Methods\n",
      "\n",
      "The content of the paragraph 38 is ：NLP Techniques\n",
      "\n",
      "The content of the paragraph 39 is ：Named Entity Recognition (NER): Identifies medical entities such as symptoms, diseases, and treatments.\n",
      "\n",
      "The content of the paragraph 40 is ：Text Classification: Categorizes input text based on symptom presence.\n",
      "\n",
      "The content of the paragraph 41 is ：Relation Extraction: Links symptoms to diseases.\n",
      "\n",
      "The content of the paragraph 42 is ：Pre-trained Models: BioBERT, ClinicalBERT for domain-specific language understanding.\n",
      "\n",
      "The content of the paragraph 43 is ：Machine Learning Approaches\n",
      "\n",
      "The content of the paragraph 44 is ：Rule-Based Systems: Use predefined symptom-disease mappings.\n",
      "\n",
      "The content of the paragraph 45 is ：Supervised Learning: Train classifiers like Random Forest or SVM for disease prediction.\n",
      "\n",
      "The content of the paragraph 46 is ：Deep Learning: Use RNNs, LSTMs, or Transformers for sequential symptom analysis.\n",
      "\n",
      "The content of the paragraph 47 is ：\n",
      "\n",
      "The content of the paragraph 48 is ：\n",
      "\n",
      "The content of the paragraph 49 is ：\n",
      "\n",
      "The content of the paragraph 50 is ：\n",
      "\n",
      "The content of the paragraph 51 is ：\n",
      "\n",
      "The content of the paragraph 52 is ：\n",
      "\n",
      "The content of the paragraph 53 is ：\n",
      "\n",
      "The content of the paragraph 54 is ：Symptom-Disease Mapping\n",
      "\n",
      "The content of the paragraph 55 is ：Knowledge Graphs: Tools like UMLS or SNOMED-CT to connect symptoms and diseases.\n",
      "\n",
      "The content of the paragraph 56 is ：Ontology-Based Mapping: Use structured vocabularies for accurate associations.\n",
      "\n",
      "The content of the paragraph 57 is ：Datasets\n",
      "\n",
      "The content of the paragraph 58 is ：SymCat: Dataset linking symptoms to diseases.\n",
      "\n",
      "The content of the paragraph 59 is ：MIMIC-III: Medical records dataset for NLP tasks.\n",
      "\n",
      "The content of the paragraph 60 is ：PubMed: Research articles for training language models.\n",
      "\n",
      "The content of the paragraph 61 is ：UMLS: Comprehensive medical terminology resource.\n",
      "\n",
      "The content of the paragraph 62 is ：\n",
      "\n",
      "The content of the paragraph 63 is ：\n",
      "\n",
      "The content of the paragraph 64 is ：Gaps and Challenges\n",
      "\n",
      "The content of the paragraph 65 is ：Ambiguity in Symptom Reporting\n",
      "\n",
      "The content of the paragraph 66 is ：Users may report symptoms in varied forms (e.g., \"headache\" vs. \"pain in the head\").\n",
      "\n",
      "The content of the paragraph 67 is ：Synonym and polysemy challenges in symptom interpretation.\n",
      "\n",
      "The content of the paragraph 68 is ：Multilingual Support\n",
      "\n",
      "The content of the paragraph 69 is ：Limited models for non-English symptom reporting.\n",
      "\n",
      "The content of the paragraph 70 is ：Lack of Comprehensive Datasets\n",
      "\n",
      "The content of the paragraph 71 is ：Existing datasets may lack diversity or coverage for rare diseases.\n",
      "\n",
      "The content of the paragraph 72 is ：Ethical and Privacy Concerns\n",
      "\n",
      "The content of the paragraph 73 is ：Ensuring compliance with HIPAA, GDPR, and other regulations for sensitive health data.\n",
      "\n",
      "The content of the paragraph 74 is ：Real-Time Accuracy\n",
      "\n",
      "The content of the paragraph 75 is ：Achieving high accuracy in real-time with limited computational resources.\n",
      "\n",
      "The content of the paragraph 76 is ：\n",
      "\n",
      "The content of the paragraph 77 is ：\n",
      "\n",
      "The content of the paragraph 78 is ：\n",
      "\n",
      "The content of the paragraph 79 is ：\n",
      "\n",
      "The content of the paragraph 80 is ：\n",
      "\n",
      "The content of the paragraph 81 is ：\n",
      "\n",
      "The content of the paragraph 82 is ：\n",
      "\n",
      "The content of the paragraph 83 is ：\n",
      "\n",
      "The content of the paragraph 84 is ：Proposed Solution Scope\n",
      "\n",
      "The content of the paragraph 85 is ：Objectives\n",
      "\n",
      "The content of the paragraph 86 is ：Develop a scalable NLP-based system for symptom recognition.\n",
      "\n",
      "The content of the paragraph 87 is ：Use advanced machine learning models for disease prediction.\n",
      "\n",
      "The content of the paragraph 88 is ：Incorporate multilingual and conversational capabilities.\n",
      "\n",
      "The content of the paragraph 89 is ：Address privacy and ethical concerns through secure data handling. Features\n",
      "\n",
      "The content of the paragraph 90 is ：User-friendly interface for symptom input.\n",
      "\n",
      "The content of the paragraph 91 is ：Real-time processing and diagnosis.\n",
      "\n",
      "The content of the paragraph 92 is ：Feedback mechanism for iterative improvements.\n",
      "\n",
      "The content of the paragraph 93 is ：\n",
      "\n",
      "The content of the paragraph 94 is ：Conclusion\n",
      "\n",
      "The content of the paragraph 95 is ：The literature survey highlights the potential of NLP in symptom analysis and disease diagnosis. By addressing gaps in datasets, multilingual support, and real-time accuracy, the proposed system aims to improve preliminary diagnosis efficiency.\n",
      "\n",
      "The content of the paragraph 96 is ：Future work includes implementing the application and validating it through real- world testing.\n",
      "\n",
      "The content of the paragraph 97 is ：\n",
      "\n",
      "The content of the paragraph 98 is ：\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  output paragraph number and paragraph content \n",
    "for i in range(len(document.paragraphs)):\n",
    "    print(\"The content of the paragraph \"+ str(i)+\" is ：\" + document.paragraphs[i].text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0abe273d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'html_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26584\\4091738827.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Parsing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Formating the parsed html file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mstrhtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprettify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'html_doc' is not defined"
     ]
    }
   ],
   "source": [
    "#Parsing\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Formating the parsed html file\n",
    "strhtm = soup.prettify()\n",
    "\n",
    "# Print few lines\n",
    "print (strhtm[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec7b4df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45709359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4a0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
